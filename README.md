# ğŸ¤– Jarvis - Personal AI Assistant

A personal AI assistant powered by a self-hosted LLM (LLaMA via Ollama) with vector database integration (Pinecone) for knowledge retrieval and a modern React.js chatbot interface.

![Jarvis AI Assistant](https://img.shields.io/badge/AI-Powered-blue?style=for-the-badge)
![React](https://img.shields.io/badge/React-18-61DAFB?style=for-the-badge&logo=react)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?style=for-the-badge&logo=fastapi)
![Ollama](https://img.shields.io/badge/Ollama-LLaMA-purple?style=for-the-badge)

## âœ¨ Features

- ğŸ§  **Self-hosted LLM** - Run LLaMA locally via Ollama
- ğŸ“š **Knowledge Base** - Store and retrieve documents with Pinecone vector search
- ğŸ’¬ **Streaming Responses** - Real-time token streaming for natural conversation
- ğŸ¨ **Premium UI** - Modern dark theme with glassmorphism effects
- ğŸ“± **Responsive Design** - Works on desktop and mobile

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama**:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Pull LLaMA model**:
   ```bash
   ollama pull llama2
   ollama pull nomic-embed-text
   ```

3. **Get Pinecone API Key** (optional):
   - Sign up at [pinecone.io](https://www.pinecone.io)
   - Create an index and get your API key

### Installation

1. **Clone & Setup Backend**:
   ```bash
   cd backend
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   
   # Configure environment
   cp .env.example .env
   # Edit .env with your Pinecone API key (optional)
   ```

2. **Setup Frontend**:
   ```bash
   cd frontend
   npm install
   ```

### Running

1. **Start Ollama** (in terminal 1):
   ```bash
   ollama serve
   ```

2. **Start Backend** (in terminal 2):
   ```bash
   cd backend
   source venv/bin/activate
   uvicorn main:app --reload --port 8000
   ```

3. **Start Frontend** (in terminal 3):
   ```bash
   cd frontend
   npm run dev
   ```

4. **Open Browser**: http://localhost:5173

## ğŸ“ Project Structure

```
Jarvis/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_service.py      # Ollama LLM integration
â”‚       â”œâ”€â”€ embedding_service.py # Text embeddings
â”‚       â””â”€â”€ vector_service.py    # Pinecone integration
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main application
â”‚   â”‚   â”œâ”€â”€ index.css        # Premium styling
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ ChatInterface.jsx   # Chat UI
â”‚   â”‚       â”œâ”€â”€ MessageBubble.jsx   # Message display
â”‚   â”‚       â””â”€â”€ KnowledgeBase.jsx   # Knowledge management
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

Edit `backend/.env` to configure:

| Variable | Description | Default |
|----------|-------------|---------|
| `OLLAMA_MODEL` | LLM model to use | `llama2` |
| `OLLAMA_EMBEDDING_MODEL` | Embedding model | `nomic-embed-text` |
| `PINECONE_API_KEY` | Your Pinecone API key | (required for knowledge base) |
| `PINECONE_INDEX_NAME` | Name of your Pinecone index | `jarvis-knowledge` |

## ğŸ“ Usage

### Chat
Simply type your message and press Enter or click Send. Jarvis will respond using the LLM.

### Knowledge Base
1. Go to the "Knowledge Base" tab
2. Upload a `.txt` file or paste text directly
3. The content will be chunked and stored in Pinecone
4. When you chat, Jarvis will search the knowledge base for relevant context

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI, Python 3.10+
- **LLM**: Ollama (LLaMA 2/3)
- **Vector DB**: Pinecone
- **Frontend**: React 18, Vite
- **Styling**: Custom CSS with glassmorphism

## ğŸ“„ License

MIT License - Feel free to use and modify!
